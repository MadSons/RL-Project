{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Install Environment, Agent, and Libraries**:\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "igLd1nZbZjAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "l1V2DukaanTb"
      },
      "outputs": [],
      "source": [
        "# Install environment and agent\n",
        "!pip install git+https://github.com/eleurent/highway-env\n",
        "\n",
        "# TODO: we use the bleeding edge version because the current stable version does not support the latest gym>=0.21 versions. Revert back to stable at the next SB3 release.\n",
        "!pip install git+https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "# Make sure agents are installed\n",
        "!pip install git+https://github.com/eleurent/rl-agents #egg=rl-agents\n",
        "\n",
        "# Environment\n",
        "import gymnasium as gym\n",
        "import highway_env\n",
        "\n",
        "gym.register_envs(highway_env)\n",
        "\n",
        "# Pull in agents from stable baseline\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3 import common\n",
        "\n",
        "# Visualization utils\n",
        "%load_ext tensorboard\n",
        "import sys\n",
        "from tqdm.notebook import trange\n",
        "!pip install tensorboardx gym pyvirtualdisplay\n",
        "!apt-get install -y xvfb ffmpeg\n",
        "!git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\n",
        "sys.path.insert(0, '/content/HighwayEnv/scripts/')\n",
        "from utils import record_videos, show_videos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in tensorboard for DQN\n",
        "%tensorboard --logdir \"highway_dqn\"\n",
        "# Might need to hit the refresh button in the top right in order to see results #"
      ],
      "metadata": {
        "id": "fWy1Kju3a5Z8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Train the DQN Model**:\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "UBnLMoPjZ6Pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the highway environment\n",
        "env = gym.make(\"highway-fast-v0\")\n",
        "\n",
        "# Define parameters of the DQN algorithm for highway environment\n",
        "model = DQN('MlpPolicy', env,\n",
        "                policy_kwargs=dict(net_arch=[256, 256]),\n",
        "                learning_rate=5e-4,\n",
        "                buffer_size=15000,\n",
        "                learning_starts=200,\n",
        "                batch_size=32,\n",
        "                gamma=0.8,\n",
        "                train_freq=1,\n",
        "                gradient_steps=1,\n",
        "                target_update_interval=50,\n",
        "                exploration_fraction=0.7,\n",
        "                verbose=1,\n",
        "                tensorboard_log='highway_dqn')\n",
        "\n",
        "# Run DQN Model n times (Tensorboard will show all runs)\n",
        "n_runs = 1\n",
        "for i in range(n_runs):\n",
        "  model.learn(int(2e4))\n",
        "\n",
        "# Save the model\n",
        "model.save(\"DQN_highway\")"
      ],
      "metadata": {
        "id": "iYGsBcUcbAoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Evaluate the Trained Model**:\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "vHrL94zDaDd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import numpy as np\n",
        "\n",
        "# Make the highway environment\n",
        "env = gym.make(\"highway-fast-v0\")\n",
        "\n",
        "# Load the trained agent\n",
        "model = DQN.load(\"DQN_highway\", env=env)\n",
        "\n",
        "# Evaluate the model n number of times\n",
        "n_eval_episodes = 10\n",
        "reward, duration = evaluate_policy(model, env,\n",
        "                                          n_eval_episodes=n_eval_episodes,\n",
        "                                          return_episode_rewards=True,\n",
        "                                          deterministic=True)\n",
        "\n",
        "\n",
        "# Tabulate the results for easy viewing\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Define array for number of tests ran when evaluating model\n",
        "Test_Number = np.arange(1, n_eval_episodes+1)\n",
        "\n",
        "# Print the table using tabulate\n",
        "print(\"\\n\", tabulate({\"Test Number\": Test_Number,\n",
        "                \"Reward\": reward,\n",
        "                \"Duration (s)\": duration}, headers=\"keys\"))\n",
        "\n",
        "# Print the average reward\n",
        "print(\"\\nAverage Reward: \", np.mean(reward),\"\\n\")\n",
        "\n",
        "# Print success rate (duration agent lasted / max duration length)\n",
        "env_max_duration = 30 # highway-fast_v0 duration is set to 30s\n",
        "print(\"Success Rate: \", np.mean(duration)/env_max_duration*100,\"% \\n\")\n"
      ],
      "metadata": {
        "id": "D51OPIpcnesX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Run Trained Model for N Episodes, record Performance Metrics**:\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ZLI4reDHaKeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the environment w/render mode\n",
        "env = gym.make(\"highway-fast-v0\", render_mode='human')\n",
        "\n",
        "# Load the trained agent\n",
        "model = DQN.load(\"DQN_highway\", env=env)\n",
        "\n",
        "# Return the current environment and reset it\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "\n",
        "# Initialize the performance data list\n",
        "performance_data = []\n",
        "\n",
        "# Define number of episodes to be ran\n",
        "n_episodes = 20\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    done = truncated = False\n",
        "    obs, info = env.reset()\n",
        "    episode_reward = 0\n",
        "    time_steps = 0\n",
        "\n",
        "    while not (done or truncated):\n",
        "        # print(\"Obs Shape - extract function: \", obs.shape)\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        new_obs, reward, done, truncated, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        time_steps += 1\n",
        "\n",
        "        # Append the performance data\n",
        "        performance_data.append({\n",
        "            'Reward': episode_reward,\n",
        "            'Step': time_steps,\n",
        "            'Speed': info.get('speed', np.nan),\n",
        "            'Crashed': info.get('crashed', False),\n",
        "            'Collision Reward': info.get('rewards', {}).get('collision_reward', 0),\n",
        "            'Right Lane Reward': info.get('rewards', {}).get('right_lane_reward', 0),\n",
        "            'High Speed Reward': info.get('rewards', {}).get('high_speed_reward', 0),\n",
        "            'On Road Reward': info.get('rewards', {}).get('on_road_reward', 0),\n",
        "        })\n",
        "\n",
        "        obs = new_obs\n",
        "\n",
        "# Create a DataFrame with the performance data\n",
        "performance_df = pd.DataFrame(performance_data)\n",
        "\n",
        "# Calculate the metrics\n",
        "average_reward = performance_df['Reward'].mean()\n",
        "reward_stddev = performance_df['Reward'].std()\n",
        "observation_variance = performance_df['Speed'].var()\n",
        "action_variance = performance_df['Reward'].var()\n",
        "total_transitions = len(performance_df)\n",
        "crash_rate = performance_df['Crashed'].sum()/n_episodes\n",
        "average_collision_reward = performance_df['Collision Reward'].mean()\n",
        "average_right_lane_reward = performance_df['Right Lane Reward'].mean()\n",
        "average_high_speed_reward = performance_df['High Speed Reward'].mean()\n",
        "average_on_road_reward = performance_df['On Road Reward'].mean()\n",
        "reward_sparsity = (performance_df['Reward'] != 0).mean()\n",
        "transition_rewards_variance = performance_df['Reward'].var()\n",
        "\n",
        "# Print Performance Metrics\n",
        "print(\"\\nModel Performance Data:\")\n",
        "print(\"---------------------------\")\n",
        "print(f\"Average Reward: {average_reward:.2f}\")\n",
        "print(f\"Reward Standard Deviation: {reward_stddev:.2f}\")\n",
        "print(f\"Observation Variance: {observation_variance:.2f}\")\n",
        "print(f\"Action Variance: {action_variance:.2f}\")\n",
        "print(f\"Total Transitions: {total_transitions}\")\n",
        "print(f\"Crash Rate: {crash_rate:.2%}\")\n",
        "print(f\"Average Collision Reward: {average_collision_reward:.2f}\")\n",
        "print(f\"Average Right Lane Reward: {average_right_lane_reward:.2f}\")\n",
        "print(f\"Average High Speed Reward: {average_high_speed_reward:.2f}\")\n",
        "print(f\"Average On Road Reward: {average_on_road_reward:.2f}\")\n",
        "print(f\"Reward Sparsity: {reward_sparsity:.2%}\")\n",
        "print(f\"Transition Rewards Variance: {transition_rewards_variance:.2f}\")"
      ],
      "metadata": {
        "id": "cvsUTjRaG7tv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Modification of the Environment Parameters:**\n",
        "This section will take ~7 hours to run\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Yo8WUahL1Bnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in tensorboard for DQN\n",
        "%tensorboard --logdir \"highway_dqn_mod1\""
      ],
      "metadata": {
        "id": "61cjvEa5BPHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment Modification #\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import numpy as np\n",
        "\n",
        "# Array to store reward vector and success rate\n",
        "Results = []\n",
        "\n",
        "# List of different environment configurations, with multiple parameters\n",
        "env_configs = [\n",
        "    {\"lanes_count\": 2, \"vehicles_count\": 50, \"reward_speed_range\": [20, 30]},\n",
        "    {\"lanes_count\": 4, \"vehicles_count\": 50, \"reward_speed_range\": [20, 30]},\n",
        "    {\"lanes_count\": 6, \"vehicles_count\": 50, \"reward_speed_range\": [20, 30]},\n",
        "    {\"lanes_count\": 2, \"vehicles_count\": 100, \"reward_speed_range\": [20, 30]},\n",
        "    {\"lanes_count\": 4, \"vehicles_count\": 100, \"reward_speed_range\": [20, 30]},\n",
        "    {\"lanes_count\": 6, \"vehicles_count\": 100, \"reward_speed_range\": [20, 30]},\n",
        "    {\"lanes_count\": 2, \"vehicles_count\": 25, \"reward_speed_range\": [40, 50]},\n",
        "    {\"lanes_count\": 4, \"vehicles_count\": 25, \"reward_speed_range\": [40, 50]},\n",
        "    {\"lanes_count\": 6, \"vehicles_count\": 25, \"reward_speed_range\": [40, 50]},\n",
        "]\n",
        "\n",
        "# Loop through the environment configurations\n",
        "for config in env_configs:\n",
        "    # Create the environment with the current config\n",
        "    env = gym.make(\"highway-fast-v0\", render_mode='human', config=config)\n",
        "\n",
        "    # Define parameters of the DQN algorithm for highway environment\n",
        "    model = DQN('MlpPolicy', env,\n",
        "                    policy_kwargs=dict(net_arch=[256, 256]),\n",
        "                    learning_rate=5e-4,\n",
        "                    buffer_size=15000,\n",
        "                    learning_starts=200,\n",
        "                    batch_size=32,\n",
        "                    gamma=0.8,\n",
        "                    train_freq=1,\n",
        "                    gradient_steps=1,\n",
        "                    target_update_interval=50,\n",
        "                    exploration_fraction=0.7,\n",
        "                    verbose=1,\n",
        "                    tensorboard_log='highway_dqn_mod1')\n",
        "\n",
        "    # Run DQN Model n times (Tensorboard will show all runs)\n",
        "    timesteps = 1e4\n",
        "    model.learn(int(timesteps))\n",
        "\n",
        "    # Evaluate the model n number of times\n",
        "    n_eval_episodes = 10\n",
        "    reward, duration = evaluate_policy(model, env,\n",
        "                                              n_eval_episodes=n_eval_episodes,\n",
        "                                              return_episode_rewards=True,\n",
        "                                              deterministic=True)\n",
        "\n",
        "    # Print the average reward\n",
        "    avg_reward = np.mean(reward)\n",
        "\n",
        "    # Print success rate (duration agent lasted / max duration length)\n",
        "    env_max_duration = 30 # highway-fast_v0 duration is set to 30s\n",
        "    success_rate = np.mean(duration)/env_max_duration*100\n",
        "\n",
        "    Results.append({\n",
        "        \"config\": config,\n",
        "        \"avg_reward\": avg_reward,\n",
        "        \"success_rate\": success_rate\n",
        "    })\n",
        "\n",
        "    # Reset the environment\n",
        "    env.reset()\n",
        "\n",
        "# Print the results\n",
        "print(Results)"
      ],
      "metadata": {
        "id": "52nqmwBJ--v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Record Episode**:\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "m_gV312-ajaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record 3 episodes of the trained agent\n",
        "env = gym.make(\"highway-fast-v0\", render_mode='rgb_array', config=config)\n",
        "env = record_videos(env)\n",
        "\n",
        "for episode in trange(3, desc='Test episodes'):\n",
        "    (obs, info), done = env.reset(), False\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, truncated, info = env.step(int(action))\n",
        "env.close()\n",
        "show_videos()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KZu4_9U__L6Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}